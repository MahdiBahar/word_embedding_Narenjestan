{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228a9b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18875 entries, 0 to 18874\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   سوال     18875 non-null  object\n",
      " 1   سطح سوم  18874 non-null  object\n",
      " 2   تعداد    18875 non-null  int64 \n",
      " 3   جواب     18874 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 590.0+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('/home/mahdi/word_embedding_Narenjestan/dataset/narenjestan_khowledgebase_editable.xlsx')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58fe8c",
   "metadata": {},
   "source": [
    "## Merged and generate txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a9d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Excel: ['سوال', 'جواب']\n",
      "Merged text file saved as 'output.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('/home/mahdi/word_embedding_Narenjestan/dataset/narenjestan_khowledgebase_editable.xlsx')\n",
    "df = data.drop(['سطح سوم' , 'تعداد'], axis=1)\n",
    "\n",
    "print(\"Columns in Excel:\", df.columns.tolist())\n",
    "\n",
    "col1 = \"سوال\"\n",
    "col2 = \"جواب\"\n",
    "\n",
    "merged_series = df[col1].astype(str) + \" \" + df[col2].astype(str)\n",
    "\n",
    "merged_series.to_csv(\"MEC-merge_Narenjestan_khnowledgebase-V0.1.txt\", index=False, header=False)\n",
    "\n",
    "print(\"Merged text file saved as 'output.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2c85a",
   "metadata": {},
   "source": [
    "## Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer, WordTokenizer, Stemmer, stopwords_list\n",
    "\n",
    "normalizer = Normalizer()\n",
    "tokenizer = WordTokenizer()\n",
    "stemmer = Stemmer()\n",
    "persian_stopwords = set(stopwords_list())\n",
    "\n",
    "def hazm_tokenizer(doc):\n",
    "    # Normalize the entire document\n",
    "    doc_norm = normalizer.normalize(doc)\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(doc_norm)\n",
    "    # Keep only alphabetic tokens (drop punctuation, numbers)\n",
    "    words = [t for t in tokens if t.isalpha()]\n",
    "    # Drop stop-words\n",
    "    words = [w for w in words if w not in persian_stopwords]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8322a5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18875 entries, 0 to 18874\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   سوال    18875 non-null  object\n",
      " 1   جواب    18874 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 295.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76593279",
   "metadata": {},
   "source": [
    "#### All documents in one merged file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a914ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "Number of unique tokens after filtering: 5342\n",
      "Top 100 terms in doc 0: [('پردازش', 0.3751819401139471), ('شماره', 0.3254274087081434), ('بررسی', 0.3107954290554311), ('جهت', 0.2789357072546294), ('اطلاعات', 0.2223509851486994), ('خطای', 0.21820356574267788), ('گروه', 0.21228696742919262), ('فنی', 0.2088356184129929), ('ارجاع', 0.2073129644352577), ('دریافت', 0.2032380523615093), ('حساب', 0.16683937156041131), ('پیغام', 0.15900857967491613), ('طریق', 0.1435210135013812), ('مشتری', 0.1389675530346303), ('گزارش', 0.13418206910460545), ('صورت', 0.13399355004069538), ('درخواست', 0.12159479622199468), ('سامانه', 0.10964558786338721), ('کارت', 0.09985709800651824), ('چک', 0.09357796303166746), ('رفع', 0.09189579292293146), ('ثبت', 0.09095319760338111), ('استعلام', 0.08896649669909808), ('وضعیت', 0.08339793358052372), ('مشکل', 0.08046863735669034), ('کد', 0.07735082206894689), ('قرارداد', 0.07429101264702191), ('اقدام', 0.06853393046453748), ('مبلغ', 0.06255932628523375), ('تاریخ', 0.060065074055039), ('کاربر', 0.0592964963329441), ('تسهیلات', 0.058846950872850856), ('نمایش', 0.05742580716029803), ('مشتریان', 0.05451101240291926), ('شعبه', 0.05397445814409829), ('بانک', 0.048579912622979386), ('بروزرسانی', 0.04250379817849331), ('صورتی', 0.03876241983320117), ('صدور', 0.03783432598010544), ('شناسه', 0.03507904735372751), ('اسناد', 0.03465850482654351), ('حقوقی', 0.03267180392226047), ('ملی', 0.03243978045898654), ('ارسال', 0.03228026432798571), ('تایید', 0.032164252596348746), ('مانده', 0.03091712648125136), ('پیشنهاد', 0.030424076621794256), ('مطابق', 0.029336466637697702), ('برداشت', 0.02885791824469522), ('حواله', 0.028741906513058253), ('حقیقی', 0.026856715873957562), ('بانکی', 0.026769707075229836), ('انجام', 0.026305660148681975), ('برگشت', 0.026175146950590387), ('اصلاح', 0.025377566295586247), ('کلید', 0.025348563362677008), ('همراه', 0.025131041365857695), ('اطلاعیه', 0.025131041365857695), ('بانکداری', 0.023912918183669557), ('نام', 0.02357938445521328), ('عملیات', 0.023477874190030935), ('واریز', 0.02318784486093852), ('اعتبار', 0.023086334595756177), ('سریال', 0.023057331662846934), ('پرداخت', 0.02297032286411921), ('مشاهده', 0.021679692349657966), ('متمرکز', 0.021056129292109274), ('سری', 0.02057758089910679), ('ضمانتنامه', 0.02036005890228748), ('مجوز', 0.019736495844738793), ('سپرده', 0.01972199437828417), ('ورود', 0.019591481180192584), ('مرکزی', 0.019315953317554792), ('افتتاح', 0.01915643718655396), ('ریز', 0.019083929854280857), ('ملت', 0.019025923988462375), ('واحد', 0.018634384394187617), ('انتقال', 0.018619882927732995), ('ذکر', 0.018590879994823756), ('مجدد', 0.018561877061914513), ('شعب', 0.01848936972964141), ('صورتیکه', 0.018039824269548168), ('مرکز', 0.01788030813854734), ('مسدودی', 0.017604780275909546), ('تحویل', 0.017561275876545684), ('خدمات', 0.017546774410091063), ('کاربران', 0.01753227294363644), ('فعال', 0.01753227294363644), ('انتخاب', 0.017401759745544854), ('اعتباری', 0.017329252413271753), ('بازپرداخت', 0.01727124654745327), ('وثیقه', 0.017198739215180166), ('کارمزد', 0.017053724550633958), ('مصوبه', 0.016981217218360857), ('رمز', 0.01685070402026927), ('الکترونیک', 0.01669118788926844), ('مذکور', 0.016140132163992855), ('توسط', 0.015966114566537404), ('تمدید', 0.01586460430135506), ('ردیف', 0.01585010283490044)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Read all lines from the merged file (each line is one document)\n",
    "with open(\"MEC-merge_Narenjestan_khnowledgebase-V0.1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    # documents = [line.strip() for line in f if line.strip()]\n",
    "    full_text = f.read()\n",
    "\n",
    "# Wrap it in a list so that vectorizer sees exactly one document\n",
    "documents = [full_text]\n",
    "\n",
    "\n",
    "# Initialize TF-IDF vectorizer (example for English; adapt tokenizer for Persian if needed)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=hazm_tokenizer,\n",
    "    lowercase=False,      # Hazm’s Normalizer already handles lowercasing/unifying forms\n",
    "    preprocessor=None,    # we do all normalization inside hazm_tokenizer\n",
    "    token_pattern=None,   # disable scikit-learn’s default regex tokenizer\n",
    "    min_df=1,             # drop tokens that appear in fewer than 2 documents\n",
    "    max_df=1.0           # drop tokens that appear in >85% of docs\n",
    ")\n",
    "\n",
    "# Fit on all documents at once\n",
    "X_tfidf = vectorizer.fit_transform(documents)\n",
    "# X_tfidf now has shape (n_docs × n_terms)\n",
    "\n",
    "# Inspect vocabulary size and top tokens per document\n",
    "print(\"Number of documents:\", len(documents))\n",
    "print(\"Number of unique tokens after filtering:\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "# Example\n",
    "import numpy as np\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense0 = X_tfidf[0].todense().A1\n",
    "top100_idx = np.argsort(dense0)[-100:][::-1]\n",
    "print(\"Top 100 terms in doc 0:\", [(feature_names[i],dense0[i]) for i in top100_idx])\n",
    "\n",
    "\n",
    "# Extract nonzero (term_index, score) pairs for this single document\n",
    "row0 = X_tfidf[0].tocoo()                   # sparse row in COO format\n",
    "nonzero_pairs = list(zip(row0.col, row0.data))\n",
    "nonzero_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Write each (term, score) tuple to a text file, one per line\n",
    "with open(\"tfidf_results.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for idx, score in nonzero_pairs:      # <-- iterate over nonzero_pairs, not dense0\n",
    "        term = feature_names[idx]\n",
    "        out_file.write(f\"{term}\\t{score:.6f}\\n\")\n",
    "\n",
    "print(\"Saved TF-IDF results to 'tfidf_results.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56dbb7",
   "metadata": {},
   "source": [
    "#### consider each row as one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c78db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (18875, 3350)\n",
      "Vocabulary size: 3350\n",
      "Saved per-token aggregated TF-IDF to 'global_tfidf_per_token.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# with open(\"MEC-merge_Narenjestan_khnowledgebase-V0.1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     documents = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "documents = merged_series \n",
    "\n",
    "# documents = merged_series\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=hazm_tokenizer,\n",
    "    lowercase=False,      # Hazm’s Normalizer already handles lowercasing/unifying forms\n",
    "    preprocessor=None,    # we do all normalization inside hazm_tokenizer\n",
    "    token_pattern=None,   # disable scikit-learn’s default regex tokenizer\n",
    "    min_df=2,             # drop tokens that appear in fewer than 2 documents\n",
    "    max_df=0.85           # drop tokens that appear in >85% of docs\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(documents)\n",
    "# X_tfidf.shape == (n_rows, n_unique_tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "print(\"Vocabulary size:\", len(feature_names))\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Convert to CSC to efficiently sum over rows for each column\n",
    "X_csc = X_tfidf.tocsc()\n",
    "\n",
    "# Sum TF-IDF weights column by column\n",
    "# This yields a NumPy array of length n_tokens, where each entry is sum of column i.\n",
    "global_tfidf_sum = np.array(X_csc.sum(axis=0)).ravel()\n",
    "\n",
    "# Pair each token with its summed TF-IDF and sort descending\n",
    "token_and_sum = list(zip(feature_names, global_tfidf_sum))\n",
    "token_and_sum.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Write results to a file: one \"token<TAB>global_tfidf_sum\" per line\n",
    "with open(\"tfidf_results_pertoken-V0.1.txt\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "    out_f.write(\"token\\tglobal_tfidf_sum\\n\")\n",
    "    for tok, score in token_and_sum:\n",
    "        out_f.write(f\"{tok}\\t{score:.6f}\\n\")\n",
    "\n",
    "print(\"Saved per-token aggregated TF-IDF to 'global_tfidf_per_token.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac325383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens (after stemming filtering): 3350\n",
      "\n",
      "Document #0 top 5 terms:\n",
      "  شناسه: 0.4651\n",
      "  انتقالی: 0.3406\n",
      "  مانده: 0.2957\n",
      "  کاربر: 0.2460\n",
      "  کدملی: 0.2267\n",
      "\n",
      "Document #1 top 5 terms:\n",
      "  مسائل: 0.4805\n",
      "  تماس: 0.3820\n",
      "  رمز: 0.3500\n",
      "  بازنشستگان: 0.2461\n",
      "  پیگیری: 0.2169\n",
      "\n",
      "Document #2 top 5 terms:\n",
      "  بازخرید: 0.4430\n",
      "  گواهی: 0.3701\n",
      "  فروش: 0.3204\n",
      "  ردیف: 0.3021\n",
      "  سپرده: 0.2863\n",
      "\n",
      "Document #3 top 5 terms:\n",
      "  حقوقی: 0.5224\n",
      "  کارت: 0.4340\n",
      "  مشتری: 0.2850\n",
      "  انتخاب: 0.2132\n",
      "  نماینده: 0.1993\n",
      "\n",
      "Document #4 top 5 terms:\n",
      "  چک: 0.4634\n",
      "  کلیه: 0.2564\n",
      "  درخواست: 0.2271\n",
      "  وضعیت: 0.2097\n",
      "  تحویل: 0.2018\n",
      "\n",
      "Document #5 top 5 terms:\n",
      "  ازدواج: 0.4920\n",
      "  مرکزی: 0.3339\n",
      "  بانک: 0.2587\n",
      "  درخواست: 0.2494\n",
      "  سامانه: 0.2334\n",
      "\n",
      "Document #6 top 5 terms:\n",
      "  کارت: 0.5201\n",
      "  درخواست: 0.3295\n",
      "  تحویل: 0.2929\n",
      "  حذف: 0.2513\n",
      "  ارسال: 0.2346\n",
      "\n",
      "Document #7 top 5 terms:\n",
      "  شهاب: 0.4350\n",
      "  سو: 0.3876\n",
      "  چک: 0.3387\n",
      "  برگشتی: 0.2615\n",
      "  استعلام: 0.2017\n",
      "\n",
      "Document #8 top 5 terms:\n",
      "  ا: 0.3883\n",
      "  مسدودی: 0.3320\n",
      "  مرکزی: 0.2943\n",
      "  بانک: 0.2280\n",
      "  برگشتی: 0.2155\n",
      "\n",
      "Document #9 top 5 terms:\n",
      "  الکترونیک: 0.3668\n",
      "  خدمات: 0.3613\n",
      "  مشکلات: 0.2850\n",
      "  شهرستان: 0.2574\n",
      "  تهران: 0.2549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "with open(\"MEC-merge_Narenjestan_khnowledgebase-V0.1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    documents = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# documents = merged_series\n",
    "\n",
    "# Build & fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=hazm_tokenizer,\n",
    "    lowercase=False,      # Hazm’s Normalizer already handles lowercasing/unifying forms\n",
    "    preprocessor=None,    # we do all normalization inside hazm_tokenizer\n",
    "    token_pattern=None,   # disable scikit-learn’s default regex tokenizer\n",
    "    min_df=2,             # drop tokens that appear in fewer than 2 documents\n",
    "    max_df=0.85           # drop tokens that appear in >85% of docs\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(documents)\n",
    "# X_tfidf is a scipy.sparse matrix of shape (n_docs × n_terms)\n",
    "\n",
    "\n",
    "# Extract vocabulary\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# feature_names[i] is the i-th token (stemmed Persian word)\n",
    "\n",
    "print(f\"Total unique tokens (after stemming filtering): {len(feature_names)}\")\n",
    "# e.g. “Total unique tokens …: 12,345”\n",
    "\n",
    "\n",
    "# Find top‐N highest TF-IDF terms per document\n",
    "def top_n_terms_for_doc(doc_vector, feature_names, n=5):\n",
    "    \n",
    "    # Convert sparse 1×n_terms row to a 1D numpy array\n",
    "    dense = doc_vector.todense().A1\n",
    "    # Get indices of the top‐n weights\n",
    "    top_idxs = np.argsort(dense)[-n:][::-1]\n",
    "    return [(feature_names[i], float(dense[i])) for i in top_idxs]\n",
    "\n",
    "# Example: print top 5 terms for the first 5 documents\n",
    "for doc_idx in range(10):\n",
    "    doc_vec = X_tfidf[doc_idx]  \n",
    "    top_terms = top_n_terms_for_doc(doc_vec, feature_names, n=5)\n",
    "    print(f\"\\nDocument #{doc_idx} top 5 terms:\")\n",
    "    for term, score in top_terms:\n",
    "        print(f\"  {term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc886c",
   "metadata": {},
   "source": [
    "## NGRAM (bigram and Trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09df3b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (18875, 85841)\n",
      "Vocabulary size: 85841\n",
      "TF-IDF matrix shape: (18875, 85841)\n",
      "Number of unique uni-gram/2-gram/3-gram tokens: 85841\n",
      "Saved corpus-wide TF-IDF for bigrams/trigrams to 'global_tfidf_ngrams.txt'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# with open(\"MEC-merge_Narenjestan_khnowledgebase-V0.1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     documents = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "documents = merged_series \n",
    "\n",
    "# documents = merged_series\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=hazm_tokenizer,\n",
    "    lowercase=False,     # Hazm normalizer already lowercases\n",
    "    preprocessor=None,\n",
    "    token_pattern=None,  # rely solely on hazm_tokenizer\n",
    "    ngram_range=(1,3),  #  unigrams , bigram and trigrams\n",
    "    min_df=2,            # drop n-grams that appear in fewer than 2 docs\n",
    "    max_df=0.85          # drop n-grams that appear in >85% of docs\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(documents)\n",
    "# X_tfidf.shape == (n_rows, n_unique_tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "print(\"Vocabulary size:\", len(feature_names))\n",
    "\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(documents)\n",
    "# Now X_tfidf has shape (18000 rows, n_ngrams columns)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "print(\"Number of unique uni-gram/2-gram/3-gram tokens:\", len(feature_names))\n",
    "\n",
    "# -----------------------------\n",
    "# Convert to CSC to efficiently sum over rows for each column\n",
    "X_csc = X_tfidf.tocsc()\n",
    "\n",
    "# Sum TF-IDF weights column by column\n",
    "# This yields a NumPy array of length n_tokens, where each entry is sum of column i.\n",
    "global_tfidf_sum = np.array(X_csc.sum(axis=0)).ravel()\n",
    "\n",
    "\n",
    "# Pair each n-gram with its aggregated TF-IDF\n",
    "token_and_score = list(zip(feature_names, global_tfidf_sum))\n",
    "token_and_score.sort(key=lambda x: x[1], reverse=True)  # sort by descending score\n",
    "\n",
    "# Write to a file, one \"n-gram<TAB>score\" per line\n",
    "with open(\"tfidf_results_pertoken_123gram-V0.2.txt\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "    out_f.write(\"ngram\\tglobal_tfidf_sum\\n\")\n",
    "    for ngram, score in token_and_score:\n",
    "        out_f.write(f\"{ngram}\\t{score:.6f}\\n\")\n",
    "\n",
    "print(\"Saved corpus-wide TF-IDF for bigrams/trigrams to 'global_tfidf_ngrams.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20eb63d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sum for 'شماره' (unigrams only): 34064.71289657068\n",
      "Raw sum for 'شماره' (uni+bi-grams): 34064.71289657068\n"
     ]
    }
   ],
   "source": [
    "### Test to see the difference between unigram only and unigram and bigram without normalization\n",
    "# Unigrams-only, no normalization\n",
    "vec1 = TfidfVectorizer(ngram_range=(1,1), tokenizer=hazm_tokenizer,\n",
    "                       token_pattern=None, lowercase=False,\n",
    "                       use_idf=True, norm=None)\n",
    "X1 = vec1.fit_transform(documents)\n",
    "\n",
    "# Unigrams+bi-grams, no normalization\n",
    "vec2 = TfidfVectorizer(ngram_range=(1,2), tokenizer=hazm_tokenizer,\n",
    "                       token_pattern=None, lowercase=False,\n",
    "                       use_idf=True, norm=None)\n",
    "X2 = vec2.fit_transform(documents)\n",
    "\n",
    "# Now check the raw TFxIDF for \"شماره\":\n",
    "idx1 = vec1.vocabulary_.get(\"شماره\")\n",
    "idx2 = vec2.vocabulary_.get(\"شماره\")\n",
    "raw1 = X1[:, idx1].sum()   # sum of TF×IDF across all docs\n",
    "raw2 = X2[:, idx2].sum()   # sum of TF×IDF across all docs\n",
    "\n",
    "print(\"Raw sum for 'شماره' (unigrams only):\", raw1)\n",
    "print(\"Raw sum for 'شماره' (uni+bi-grams):\", raw2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7f5c7",
   "metadata": {},
   "source": [
    "## Count unique unigram, bigram and trigram terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2408dca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 n-grams by count:\n",
      "         ngram  count\n",
      "        پردازش  25872\n",
      "         شماره  22441\n",
      "         بررسی  21432\n",
      "           جهت  19235\n",
      "       اطلاعات  15333\n",
      "          خطای  15047\n",
      "          گروه  14639\n",
      "           فنی  14401\n",
      "         ارجاع  14296\n",
      "      گروه فنی  14286\n",
      "     فنی ارجاع  14198\n",
      "گروه فنی ارجاع  14193\n",
      "        دریافت  14015\n",
      "     جهت بررسی  13914\n",
      "    بررسی گروه  13852\n",
      "بررسی گروه فنی  13850\n",
      "جهت بررسی گروه  13845\n",
      "          حساب  11505\n",
      "         پیغام  10965\n",
      "    پیغام خطای  10339\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read the corpus (one document per line)\n",
    "\n",
    "documents = merged_series \n",
    "\n",
    "# Create a CountVectorizer to count unigrams, bigrams, and trigrams\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=hazm_tokenizer,\n",
    "    lowercase=False,     # Hazm already handles casing\n",
    "    token_pattern=None,  # disable sklearn’s default regex\n",
    "    ngram_range=(1, 3)   # unigrams, bigrams, trigrams\n",
    ")\n",
    "\n",
    "# 4. Fit and transform to get a document-term matrix of counts\n",
    "X_counts = vectorizer.fit_transform(documents)  # shape: (n_docs, n_ngrams)\n",
    "\n",
    "# 5. Sum counts over all documents for each n-gram (column)\n",
    "#    Convert to CSC for efficient column sum\n",
    "X_csc = X_counts.tocsc()\n",
    "ngram_counts = np.array(X_csc.sum(axis=0)).ravel()  # length = number of n-grams\n",
    "\n",
    "# 6. Invert the vocabulary mapping to get index → n-gram\n",
    "vocab_inv = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# 7. Create a DataFrame of (ngram, count) and sort descending\n",
    "ngram_list = [(vocab_inv[i], ngram_counts[i]) for i in range(len(ngram_counts))]\n",
    "df_counts = pd.DataFrame(ngram_list, columns=[\"ngram\", \"count\"])\n",
    "df_counts = df_counts.sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# # 8. Save full counts to a CSV file\n",
    "# df_counts.to_csv(\"ngram_counts.csv\", index=False)\n",
    "\n",
    "# 9. (Optional) Save to a text file as \"ngram<TAB>count\"\n",
    "with open(\"ngram_counts.txt\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for _, row in df_counts.iterrows():\n",
    "        out_f.write(f\"{row['ngram']}\\t{int(row['count'])}\\n\")\n",
    "\n",
    "# 10. Display top 20 n-grams to the user\n",
    "# 10. Print the top 20 n-grams\n",
    "print(\"\\nTop 20 n-grams by count:\")\n",
    "print(df_counts.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7243d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
