{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('/home/mahdi/word_embedding_Narenjestan/dataset/narenjestan_khowledgebase_editable.xlsx')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef2e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(['سطح سوم' , 'تعداد'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e929764e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18875 entries, 0 to 18874\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   سوال    18875 non-null  object\n",
      " 1   جواب    18874 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 295.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252247df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>سوال</th>\n",
       "      <th>جواب</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>در پردازش 717 (تبادل مانده انتقالی کاربر) پیغا...</td>\n",
       "      <td>در تمامی فعالیتهایی که منجر به ایجاد مانده انت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>جهت پیگیری مسائل مربوط به امور بازنشستگان بانک...</td>\n",
       "      <td>در صورتی که رمز ورود به سامانه فراموش شده است،...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بازخرید گواهی سپرده به صورت سیستمی انجام شده، ...</td>\n",
       "      <td>پردازش 911 (گزارش صورتحساب سی و پنج گردش آخر) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>جهت فعال سازی اکسس کارت مشتری حقوقی از چه طریق...</td>\n",
       "      <td>ابتدا در پردازش 685 (فعال سازی کارت های مشتری ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>در پردازش 2937 (گزارش استعلام وضعیت درخواست دس...</td>\n",
       "      <td>ابتدا در پردازش 2934 (گزارش درخواست/تحویل دسته...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                سوال  \\\n",
       "0  در پردازش 717 (تبادل مانده انتقالی کاربر) پیغا...   \n",
       "1  جهت پیگیری مسائل مربوط به امور بازنشستگان بانک...   \n",
       "2  بازخرید گواهی سپرده به صورت سیستمی انجام شده، ...   \n",
       "3  جهت فعال سازی اکسس کارت مشتری حقوقی از چه طریق...   \n",
       "4  در پردازش 2937 (گزارش استعلام وضعیت درخواست دس...   \n",
       "\n",
       "                                                جواب  \n",
       "0  در تمامی فعالیتهایی که منجر به ایجاد مانده انت...  \n",
       "1  در صورتی که رمز ورود به سامانه فراموش شده است،...  \n",
       "2  پردازش 911 (گزارش صورتحساب سی و پنج گردش آخر) ...  \n",
       "3  ابتدا در پردازش 685 (فعال سازی کارت های مشتری ...  \n",
       "4  ابتدا در پردازش 2934 (گزارش درخواست/تحویل دسته...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def delete_duplicate_sentences_in_excel(input_path: str, output_path: str, drop_column: list) -> None:\n",
    "    \n",
    "    # Load the workbook into a DataFrame\n",
    "    df = pd.read_excel(input_path)\n",
    "    df.drop(drop_column, axis=1, inplace=True)\n",
    "    # Set to track sentences we've already kept\n",
    "    seen_sentences = set()\n",
    "\n",
    "    def dedup_text(text: str) -> str:\n",
    "    \n",
    "        if pd.isnull(text):\n",
    "            return text\n",
    "        # Split on punctuation followed by whitespace\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', str(text))\n",
    "        result_parts = []\n",
    "        for sentence in sentences:\n",
    "            key = sentence.strip()\n",
    "            if not key:\n",
    "                continue\n",
    "            if key not in seen_sentences:\n",
    "                seen_sentences.add(key)\n",
    "                result_parts.append(sentence)\n",
    "            else:\n",
    "                # duplicate — replace with blank\n",
    "                result_parts.append('')\n",
    "        # Rejoin, preserving order but skipping empty entries\n",
    "        return ' '.join([s for s in result_parts if s]).strip()\n",
    "\n",
    "    # Apply to every column in the DataFrame\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(dedup_text)\n",
    "\n",
    "    # Save the deduplicated DataFrame\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"Deduplication complete. Clean file saved to: {output_path}\")\n",
    "    # return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1675dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def delete_duplicate_sentences_in_excel(input_path: str,\n",
    "                                        output_path: str,\n",
    "                                        drop_column: list = None) -> None:\n",
    "\n",
    "    drop_column = drop_column or []\n",
    "\n",
    "    # Load and drop \n",
    "    df = pd.read_excel(input_path, dtype=str)\n",
    "    if drop_column:\n",
    "        df = df.drop(columns=drop_column, errors='ignore')\n",
    "    df = df.fillna('')\n",
    "\n",
    "    # Count all sentences \n",
    "    sentence_counter = Counter()\n",
    "    # same regex you were using\n",
    "    splitter = lambda text: re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "    for cell in df.values.flatten():\n",
    "        for sent in splitter(str(cell)):\n",
    "            s = sent.strip()\n",
    "            if s:\n",
    "                sentence_counter[s] += 1\n",
    "\n",
    "    # --- 3) Deduplicate, keeping first occurrence ---\n",
    "    seen = set()\n",
    "    def dedup_text(text: str) -> str:\n",
    "        parts = []\n",
    "        for sent in splitter(str(text)):\n",
    "            s = sent.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            if s not in seen:\n",
    "                seen.add(s)\n",
    "                parts.append(s)\n",
    "            # else skip (i.e. drop duplicate)\n",
    "        return ' '.join(parts)\n",
    "\n",
    "    cleaned = df.applymap(dedup_text)\n",
    "\n",
    "    # --- 4) Write both sheets ---\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        cleaned.to_excel(writer, sheet_name='Cleaned', index=False)\n",
    "\n",
    "        counts_df = (\n",
    "            pd.DataFrame.from_records(\n",
    "                list(sentence_counter.items()),\n",
    "                columns=['sentence', 'count']\n",
    "            )\n",
    "            .sort_values('count', ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        counts_df.to_excel(writer, sheet_name='Sentence Counts', index=False)\n",
    "\n",
    "    print(f\"Done! Cleaned data + sentence counts saved to '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42b871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2864178/3053966018.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  cleaned = df.applymap(dedup_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Cleaned data + sentence counts saved to '/home/mahdi/word_embedding_Narenjestan/dataset/deduplicated_sentences.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Adjust these paths as needed\n",
    "    input_file = \"/home/mahdi/word_embedding_Narenjestan/dataset/narenjestan_khowledgebase_editable.xlsx\"\n",
    "    output_file = \"/home/mahdi/word_embedding_Narenjestan/dataset/deduplicated_sentences.xlsx\"\n",
    "    delete_duplicate_sentences_in_excel(input_file, output_file, ['سطح سوم' , 'تعداد'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3fd646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def dedup_ngrams_in_excel(input_path: str,\n",
    "                          output_path: str,\n",
    "                          max_occurrences: int = 10):\n",
    "    \n",
    "    # 1) Load\n",
    "    df = pd.read_excel(input_path)\n",
    "\n",
    "    # 2) For each n-gram length\n",
    "    for n in [6, 5, 4, 3, 2]:\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        def dedup_cell(text):\n",
    "            if pd.isnull(text):\n",
    "                return text\n",
    "            # simple tokenization (words and punctuation split)\n",
    "            tokens = re.findall(r\"\\w+|[^\\w\\s]\", str(text), re.UNICODE)\n",
    "            result = list(tokens)  # copy for in-place blanking\n",
    "\n",
    "            # slide window\n",
    "            for i in range(len(tokens) - n + 1):\n",
    "                gram = tuple(tokens[i:i+n])\n",
    "                counts[gram] += 1\n",
    "                if counts[gram] > max_occurrences:\n",
    "                    # blank out this over-quota n-gram\n",
    "                    for j in range(i, i+n):\n",
    "                        result[j] = \"\"\n",
    "\n",
    "            # reconstruct: drop blanks, join with spaces\n",
    "            cleaned = \" \".join(tok for tok in result if tok)\n",
    "            return cleaned\n",
    "\n",
    "        # apply to every cell\n",
    "        df = df.applymap(dedup_cell)\n",
    "\n",
    "    # 3) Save\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"Done – cleaned file saved to '{output_path}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bb0b157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1489679/255363609.py:37: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(dedup_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – cleaned file saved to '/home/mahdi/word_embedding_Narenjestan/dataset/dedup_ngrams_output.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_file = \"/home/mahdi/word_embedding_Narenjestan/dataset/narenjestan_khowledgebase_editable.xlsx\"              # ← change to your input path\n",
    "    out_file = \"/home/mahdi/word_embedding_Narenjestan/dataset/dedup_ngrams_output.xlsx\"         # ← desired output path\n",
    "    dedup_ngrams_in_excel(in_file, out_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85996e6",
   "metadata": {},
   "source": [
    "## NGRAM with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_and_save_ngrams(input_xlsx: str,\n",
    "                            output_xlsx: str,\n",
    "                            min_n: int = 2,\n",
    "                            max_n: int = 6,\n",
    "                            drop_column: list = None,\n",
    "                            valid_count: int = 10):\n",
    "   \n",
    "    drop_column = drop_column or []\n",
    "    # 1) Load your data\n",
    "    df = pd.read_excel(input_xlsx, dtype=str).fillna('')\n",
    "    if drop_column:\n",
    "        df = df.drop(columns=drop_column, errors='ignore')\n",
    "\n",
    "    # 2) Concatenate all cells into one big lowercase string\n",
    "    full_text = df.astype(str).agg(' '.join, axis=1).str.cat(sep=' ').lower()\n",
    "\n",
    "    # 3) Tokenize on word characters\n",
    "    tokens = re.findall(r'\\w+', full_text)\n",
    "\n",
    "    # Helper: sliding-window generator\n",
    "    def ngrams(tokens, n):\n",
    "        return zip(*(tokens[i:] for i in range(n)))\n",
    "\n",
    "    # 4) Prepare an Excel writer\n",
    "    with pd.ExcelWriter(output_xlsx, engine='openpyxl') as writer:\n",
    "        for n in range(min_n, max_n + 1):\n",
    "            # Count all n-grams\n",
    "            counts = Counter(ngrams(tokens, n))\n",
    "            # Filter to only those >= valid_count\n",
    "            filtered = {gram: cnt for gram, cnt in counts.items() if cnt >= valid_count}\n",
    "            if not filtered:\n",
    "                # nothing to write for this n\n",
    "                continue\n",
    "\n",
    "            # Build a DataFrame from filtered counts\n",
    "            df_ng = (\n",
    "                pd.DataFrame(filtered.items(), columns=['ngram_tuple', 'count'])\n",
    "                  .assign(ngram=lambda d: d['ngram_tuple']\n",
    "                                         .apply(lambda tup: ' '.join(tup)))\n",
    "                  .loc[:, ['ngram', 'count']]\n",
    "                  .sort_values('count', ascending=False)\n",
    "                  .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # Write to its own sheet\n",
    "            sheet_name = f\"{n}-grams\"\n",
    "            df_ng.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"All done! N-grams (count ≥ {valid_count}) saved to '{output_xlsx}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_and_save_ngrams(\n",
    "        input_xlsx=\"/home/mahdi/word_embedding_Narenjestan/dataset/narenjestan_khowledgebase_editable.xlsx\" ,        # ← your source file\n",
    "        output_xlsx=\"ngram_counts.xlsx\",    # ← where to write results\n",
    "        min_n=2,\n",
    "        max_n=10,\n",
    "        drop_column=['سطح سوم' , 'تعداد']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180d229",
   "metadata": {},
   "source": [
    "## NGRAM with filter (using NLTK for tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ede13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using nltk\n",
    "\n",
    "# import pandas as pd\n",
    "# import nltk\n",
    "# from nltk import ngrams\n",
    "# from nltk.tokenize import wordpunct_tokenize\n",
    "# from collections import Counter\n",
    "\n",
    "# def extract_and_save_ngrams_nltk(input_xlsx: str,\n",
    "#                                  output_xlsx: str,\n",
    "#                                  min_n: int = 2,\n",
    "#                                  max_n: int = 6,\n",
    "#                                  drop_column: list = None,\n",
    "#                                  valid_count: int = 10):\n",
    "\n",
    "#     drop_column = drop_column or []\n",
    "\n",
    "#     # 1) Load data\n",
    "#     df = pd.read_excel(input_xlsx, dtype=str).fillna('')\n",
    "#     if drop_column:\n",
    "#         df = df.drop(columns=drop_column, errors='ignore')\n",
    "\n",
    "#     # 2) Combine all text into one lowercase blob\n",
    "#     full_text = df.astype(str).agg(' '.join, axis=1).str.cat(sep=' ').lower()\n",
    "\n",
    "#     # 3) Tokenize into words and punctuation (no external models)\n",
    "#     tokens = wordpunct_tokenize(full_text)\n",
    "\n",
    "#     # 4) Prepare Excel writer\n",
    "#     with pd.ExcelWriter(output_xlsx, engine='openpyxl') as writer:\n",
    "#         for n in range(min_n, max_n + 1):\n",
    "#             # Count all n-grams\n",
    "#             cnt = Counter(ngrams(tokens, n))\n",
    "#             # Keep only those with count >= valid_count\n",
    "#             filtered = {gram: c for gram, c in cnt.items() if c >= valid_count}\n",
    "#             if not filtered:\n",
    "#                 continue\n",
    "\n",
    "#             # Build DataFrame\n",
    "#             df_ng = (\n",
    "#                 pd.DataFrame(filtered.items(), columns=['ngram_tuple', 'count'])\n",
    "#                   .assign(\n",
    "#                       ngram=lambda d: d['ngram_tuple']\n",
    "#                                          .apply(lambda tup: ' '.join(tup))\n",
    "#                   )\n",
    "#                   .loc[:, ['ngram', 'count']]\n",
    "#                   .sort_values('count', ascending=False)\n",
    "#                   .reset_index(drop=True)\n",
    "#             )\n",
    "\n",
    "#             # Write to sheet named \"2-grams\", \"3-grams\", etc.\n",
    "#             sheet = f\"{n}-grams\"\n",
    "#             df_ng.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "#     print(f\"Done! N-grams (count ≥ {valid_count}) saved to '{output_xlsx}'.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     extract_and_save_ngrams_nltk(\n",
    "#         input_xlsx=\"/home/mahdi/word_embedding_Narenjestan/dataset/narenjestan_khowledgebase_editable.xlsx\" ,        # ← your source file\n",
    "#         output_xlsx=\"ngram_counts_nltk.xlsx\",    # ← where to write results\n",
    "#         min_n=2,\n",
    "#         max_n=10,\n",
    "#         drop_column=['سطح سوم' , 'تعداد']\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
